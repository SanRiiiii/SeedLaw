#!/usr/bin/env python3
"""
é‡æ–°ç´¢å¼•è„šæœ¬
ä½¿ç”¨æ–°çš„embeddingæ¨¡å‹é‡æ–°å¯¹/backend/data/chunks/related_lawsé‡Œçš„æ•°æ®è¿›è¡Œç´¢å¼•
å¹¶å­˜å…¥Milvusçš„æ–°é›†åˆfinetune_data
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))

from backend.app.db.milvus import VectorStore
from backend.app.models.Embeddings.bge_embedding import BGEEmbedding
from backend.app.core.config import Settings
from pymilvus import FieldSchema, DataType

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–é…ç½®
settings = Settings()

class VectorIndexer:
    def __init__(self):
        """åˆå§‹åŒ–å‘é‡ç´¢å¼•å™¨"""
        self.embedding_model = BGEEmbedding()
        self.vector_store = VectorStore()
        self.collection_name = "finetune_data"
        self.data_dir = Path("../backend/data/chunks/related_laws")

        
        # ç¡®ä¿embeddingæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ
        if not self.embedding_model.is_initialized:
            raise Exception("BGE embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
        
        logger.info("å‘é‡ç´¢å¼•å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_collection_fields(self) -> List[FieldSchema]:
        """å®šä¹‰Milvusé›†åˆçš„å­—æ®µç»“æ„"""
        """
        "{'auto_id': False, 'description': 'æ³•å¾‹æ–‡æ¡£åˆ†å—é›†åˆ', 
        'fields': [{'name': 'uuid', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 36}, 'is_primary': True, 'auto_id': False},
        {'name': 'content', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 4096}}, 
        {'name': 'document_name', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 256}},
        {'name': 'chapter', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 256}}, 
        {'name': 'section', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 256}}, 
        {'name': 'effective_date', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 20}}, 
        {'name': 'is_effective', 'description': '', 'type': <DataType.BOOL: 1>}, 
        {'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 1024}}], 'enable_dynamic_field': False}"
        """
        fields = [
            FieldSchema(name="uuid", dtype=DataType.VARCHAR, max_length=36, is_primary=True, auto_id=False),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=settings.EMBEDDING_DIMENSION),
            FieldSchema(name="document_name", dtype=DataType.VARCHAR, max_length=256),
            FieldSchema(name="chapter", dtype=DataType.VARCHAR, max_length=256),
            FieldSchema(name="section", dtype=DataType.VARCHAR, max_length=256),
            FieldSchema(name="effective_date", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="is_effective", dtype=DataType.BOOL),
        ]
        return fields
    
    def load_chunks_from_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """ä»JSONæ–‡ä»¶ä¸­åŠ è½½chunks"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    return data
                else:
                    logger.warning(f"æ–‡ä»¶ {file_path} ä¸æ˜¯åˆ—è¡¨ç»“æ„")
                    return []
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            return []
    
    def load_all_chunks(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ‰€æœ‰related_lawsç›®å½•ä¸‹çš„chunks"""
        all_chunks = []
        
        if not self.data_dir.exists():
            raise Exception(f"æ•°æ®ç›®å½• {self.data_dir} ä¸å­˜åœ¨")
        
        json_files = list(self.data_dir.glob("*.json"))
        logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        
        for json_file in tqdm(json_files, desc="åŠ è½½æ•°æ®æ–‡ä»¶"):
            # è·³è¿‡.DS_Storeç›¸å…³æ–‡ä»¶
            if ".DS_Store" in json_file.name:
                continue
                
            chunks = self.load_chunks_from_file(json_file)
            if chunks:
                logger.info(f"ä» {json_file.name} åŠ è½½äº† {len(chunks)} ä¸ªchunks")
                all_chunks.extend(chunks)
            else:
                logger.warning(f"æ–‡ä»¶ {json_file.name} ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆchunks")
        
        logger.info(f"æ€»å…±åŠ è½½äº† {len(all_chunks)} ä¸ªchunks")
        return all_chunks
    
    def save_embeddings_to_file(self, embeddings_data: Dict[str, Any], filename: str = "embeddings_cache.json"):
        """å°†å‘é‡åŒ–ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(embeddings_data, f, ensure_ascii=False, indent=2)
            logger.info(f"å‘é‡åŒ–ç»“æœå·²ä¿å­˜åˆ° {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡åŒ–ç»“æœå¤±è´¥: {e}")
    
    def load_embeddings_from_file(self, filename: str = "embeddings_cache.json") -> Dict[str, Any]:
        """ä»JSONæ–‡ä»¶åŠ è½½å‘é‡åŒ–ç»“æœ"""
        try:
            if os.path.exists(filename):
                with open(filename, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                logger.info(f"ä» {filename} åŠ è½½äº†å‘é‡åŒ–ç¼“å­˜")
                return data
            else:
                logger.info(f"ç¼“å­˜æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
                return None
        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡åŒ–ç¼“å­˜å¤±è´¥: {e}")
            return None

    def compute_and_cache_embeddings(self, chunks: List[Dict[str, Any]], cache_filename: str) -> List[List[float]]:
        """è®¡ç®—å‘é‡å¹¶ç¼“å­˜åˆ°æ–‡ä»¶"""
        logger.info("å¼€å§‹å‘é‡åŒ–æ–‡æœ¬å†…å®¹...")
        
        # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹å’ŒUUID
        contents = [chunk['content'] for chunk in chunks]
        uuids = [chunk['uuid'] for chunk in chunks]
        
        # åˆ†æ‰¹å‘é‡åŒ–ï¼Œå¸¦è¿›åº¦æ¡
        batch_size = 32
        total_batches = (len(contents) + batch_size - 1) // batch_size
        embedding_list = []
        
        logger.info(f"å¯¹ {len(contents)} ä¸ªæ–‡æœ¬è¿›è¡Œæ‰¹é‡å‘é‡åŒ–ï¼Œåˆ† {total_batches} æ‰¹å¤„ç†...")
        
        for i in tqdm(range(0, len(contents), batch_size), desc="å‘é‡åŒ–è¿›åº¦", unit="batch"):
            # è·å–å½“å‰æ‰¹æ¬¡çš„å†…å®¹
            batch_contents = contents[i:i + batch_size]
            
            # å¯¹å½“å‰æ‰¹æ¬¡è¿›è¡Œå‘é‡åŒ–
            batch_embeddings = self.embedding_model.encode(batch_contents, batch_size=len(batch_contents), normalize=True)
            
            # å¤„ç†embeddingæ ¼å¼
            # å¦‚æœæ˜¯å•ä¸ªå‘é‡(ä¸€ç»´æ•°ç»„)ï¼Œreshapeä¸ºäºŒç»´
            if batch_embeddings.ndim == 1:
                batch_embeddings = batch_embeddings.reshape(1, -1)
            
            # è½¬æ¢å½“å‰æ‰¹æ¬¡çš„å‘é‡ä¸ºåˆ—è¡¨æ ¼å¼
            for j in range(batch_embeddings.shape[0]):
                vector = batch_embeddings[j]
                if isinstance(vector, np.ndarray):
                    embedding_list.append(vector.astype(np.float32).tolist())
                else:
                    embedding_list.append(vector)
        
        
        # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
        cache_data = {
            'uuids': uuids,
            'embeddings': embedding_list,
            'total_count': len(chunks),
            'embedding_dimension': len(embedding_list[0]) if embedding_list else 0
        }
        self.save_embeddings_to_file(cache_data, cache_filename)
        logger.info(f"å‘é‡åŒ–å®Œæˆï¼Œå…±ç”Ÿæˆ {len(embedding_list)} ä¸ªå‘é‡")
        
        return embedding_list
    


    def prepare_entities(self, chunks: List[Dict[str, Any]]) -> List[List[Any]]:
        """å‡†å¤‡è¦æ’å…¥Milvusçš„å®ä½“æ•°æ®"""
        cache_filename = "embeddings_cache.json"
        
        # å°è¯•ä»ç¼“å­˜æ–‡ä»¶åŠ è½½å‘é‡åŒ–ç»“æœ
        cached_data = self.load_embeddings_from_file(cache_filename)
        
        if cached_data:
            logger.info("å‘ç°å‘é‡åŒ–ç¼“å­˜æ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…å½“å‰æ•°æ®...")
            # æ£€æŸ¥ç¼“å­˜çš„chunkæ•°é‡æ˜¯å¦åŒ¹é…
            if len(cached_data.get('embeddings', [])) == len(chunks):
                # éªŒè¯UUIDæ˜¯å¦åŒ¹é…
                cached_uuids = set(cached_data.get('uuids', []))
                current_uuids = set([chunk['uuid'] for chunk in chunks])
                
                if cached_uuids == current_uuids:
                    logger.info("ç¼“å­˜æ•°æ®åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜çš„å‘é‡åŒ–ç»“æœ")
                    embeddings_list = cached_data['embeddings']
                else:
                    logger.info("ç¼“å­˜æ•°æ®UUIDä¸åŒ¹é…ï¼Œé‡æ–°è®¡ç®—å‘é‡")
                    embeddings_list = self.compute_and_cache_embeddings(chunks, cache_filename)
            else:
                logger.info("ç¼“å­˜æ•°æ®æ•°é‡ä¸åŒ¹é…ï¼Œé‡æ–°è®¡ç®—å‘é‡")
                embeddings_list = self.compute_and_cache_embeddings(chunks, cache_filename)
        else:
            logger.info("æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œå¼€å§‹è®¡ç®—å‘é‡...")
            embeddings_list = self.compute_and_cache_embeddings(chunks, cache_filename)
        
        # å‡†å¤‡å®ä½“æ•°æ®
        entities = [
            [],  # uuid
            [],  # content
            [],  # embedding
            [],  # document_name
            [],  # chapter
            [],  # section
            [],  # effective_date
            [],  # is_effective
        ]
        
        for i, chunk in enumerate(tqdm(chunks, desc="å‡†å¤‡å®ä½“æ•°æ®")):
            # è·å–metadataï¼Œæä¾›é»˜è®¤å€¼
            metadata = chunk.get('metadata', {})
            
            entities[0].append(chunk['uuid'])
            entities[1].append(chunk['content'])  # é™åˆ¶é•¿åº¦
            entities[2].append(embeddings_list[i])  # ä»ç¼“å­˜æˆ–æ–°è®¡ç®—çš„å‘é‡åˆ—è¡¨ä¸­è·å–
            entities[3].append(metadata.get('document_name', ''))
            entities[4].append(metadata.get('chapter', ''))
            entities[5].append(metadata.get('section', ''))
            entities[6].append(str(metadata.get('effective_date', '')))
            entities[7].append(metadata.get('is_effective', True))
        
        logger.info(f"å‡†å¤‡äº† {len(entities[0])} æ¡å®ä½“æ•°æ®")
        return entities
    
    def create_entities_from_cache(self, chunks: List[Dict[str, Any]], cache_filename: str = "embeddings_cache.json") -> List[List[Any]]:
        """ç›´æ¥ä»ç¼“å­˜æ–‡ä»¶åˆ›å»ºå®ä½“ï¼Œæ£€æŸ¥contenté•¿åº¦"""
        
        # åŠ è½½ç¼“å­˜çš„å‘é‡
        cached_data = self.load_embeddings_from_file(cache_filename)
        if not cached_data:
            raise Exception("æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œå‘é‡åŒ–")
        
        embeddings_list = cached_data['embeddings']
        cached_uuids = cached_data['uuids']
        
        logger.info(f"ä»ç¼“å­˜åŠ è½½äº† {len(embeddings_list)} ä¸ªå‘é‡")
        
        # éªŒè¯æ•°æ®åŒ¹é…
        current_uuids = [chunk['uuid'] for chunk in chunks]
        if set(cached_uuids) != set(current_uuids):
            logger.warning("ç¼“å­˜UUIDä¸å½“å‰æ•°æ®ä¸å®Œå…¨åŒ¹é…")
        
        # åˆ›å»ºUUIDåˆ°embeddingçš„æ˜ å°„
        uuid_to_embedding = {uuid: emb for uuid, emb in zip(cached_uuids, embeddings_list)}
        
        # å‡†å¤‡å®ä½“æ•°æ®
        entities = [
            [],  # uuid
            [],  # content
            [],  # embedding
            [],  # document_name
            [],  # chapter
            [],  # section
            [],  # effective_date
            [],  # is_effective
        ]
        
        problem_items = []
        
        for i, chunk in enumerate(tqdm(chunks, desc="æ£€æŸ¥å¹¶å‡†å¤‡å®ä½“æ•°æ®")):
            metadata = chunk.get('metadata', {})
            content = chunk['content']
            
            # æ£€æŸ¥contenté•¿åº¦
            content_char_len = len(content)
            content_byte_len = len(content.encode('utf-8'))
            
            if content_char_len > 32768 or content_byte_len > 65535:
                problem_item = {
                    'index': i,
                    'uuid': chunk['uuid'],
                    'content_char_length': content_char_len,
                    'content_byte_length': content_byte_len,
                    'content_preview': content[:200] + "..." if len(content) > 200 else content,
                    'document_name': metadata.get('document_name', ''),
                    'chapter': metadata.get('chapter', ''),
                    'section': metadata.get('section', ''),
                }
                problem_items.append(problem_item)
        
        if problem_items:
            print(f"\nğŸš¨ å‘ç° {len(problem_items)} ä¸ªcontentè¶…é•¿çš„é¡¹ç›®:")
            for idx, item in enumerate(problem_items[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"\n--- é—®é¢˜é¡¹ç›® {idx+1} ---")
                print(f"Index: {item['index']}")
                print(f"UUID: {item['uuid']}")
                print(f"å­—ç¬¦é•¿åº¦: {item['content_char_length']}")
                print(f"å­—èŠ‚é•¿åº¦: {item['content_byte_length']}")
                print(f"æ–‡æ¡£: {item['document_name']}")
                print(f"ç« èŠ‚: {item['chapter']}")
                print(f"å°èŠ‚: {item['section']}")
                print(f"å†…å®¹é¢„è§ˆ: {item['content_preview']}")
            
            if len(problem_items) > 10:
                print(f"\n... è¿˜æœ‰ {len(problem_items) - 10} ä¸ªç±»ä¼¼é—®é¢˜")
            
            user_choice = input(f"\næ˜¯å¦ç»§ç»­å¤„ç†ï¼Ÿä¼šè‡ªåŠ¨æˆªæ–­è¶…é•¿content (y/N): ")
            if user_choice.lower() != 'y':
                print("æ“ä½œå·²å–æ¶ˆ")
                return None
        
        # ç»§ç»­å¤„ç†ï¼Œä¸å†éœ€è¦æˆªæ–­content
        for i, chunk in enumerate(tqdm(chunks, desc="åˆ›å»ºå®ä½“æ•°æ®")):
            metadata = chunk.get('metadata', {})
            content = chunk['content']
            
            # è·å–å¯¹åº”çš„embedding
            embedding = uuid_to_embedding.get(chunk['uuid'])
            if embedding is None:
                logger.warning(f"æœªæ‰¾åˆ°UUID {chunk['uuid']} çš„embeddingï¼Œè·³è¿‡")
                continue
            
            entities[0].append(chunk['uuid'])
            entities[1].append(content)
            entities[2].append(embedding)
            entities[3].append(metadata.get('document_name', ''))
            entities[4].append(metadata.get('chapter', ''))
            entities[5].append(metadata.get('section', ''))
            entities[6].append(str(metadata.get('effective_date', '')))
            entities[7].append(metadata.get('is_effective', True))
        
        logger.info(f"å‡†å¤‡äº† {len(entities[0])} æ¡å®ä½“æ•°æ®")
        return entities

    def create_and_populate_collection(self, chunks: List[Dict[str, Any]]):
        """åˆ›å»ºé›†åˆå¹¶å¡«å……æ•°æ®"""
        # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
        if self.vector_store.check_collection_exists(self.collection_name):
            logger.warning(f"é›†åˆ {self.collection_name} å·²å­˜åœ¨")
            user_input = input("æ˜¯å¦è¦åˆ é™¤ç°æœ‰é›†åˆå¹¶é‡æ–°åˆ›å»ºï¼Ÿ(y/N): ")
            if user_input.lower() == 'y':
                self.vector_store.drop_collection(self.collection_name)
                logger.info(f"å·²åˆ é™¤ç°æœ‰é›†åˆ {self.collection_name}")
            else:
                logger.info("æ“ä½œå·²å–æ¶ˆ")
                return
        
        # åˆ›å»ºé›†åˆ
        fields = self.create_collection_fields()
        description = "ä½¿ç”¨æ–°embeddingæ¨¡å‹é‡æ–°ç´¢å¼•çš„æ³•å¾‹æ–‡æ¡£é›†åˆ"
        
        logger.info(f"åˆ›å»ºé›†åˆ {self.collection_name}...")
        collection = self.vector_store.create_collection(
            fields=fields,
            collection_name=self.collection_name,
            description=description
        )
        
        if not collection:
            raise Exception("åˆ›å»ºé›†åˆå¤±è´¥")
        
        # å‡†å¤‡æ•°æ® - ç›´æ¥ä»ç¼“å­˜åˆ›å»ºå®ä½“
        entities = self.create_entities_from_cache(chunks)
        
        if entities is None:
            logger.info("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return
        
        # æ‰¹é‡æ’å…¥æ•°æ®
        batch_size = 1000
        total_entities = len(entities[0])
        
        logger.info(f"å¼€å§‹æ‰¹é‡æ’å…¥æ•°æ®ï¼Œæ€»å…± {total_entities} æ¡è®°å½•...")
        
        for i in tqdm(range(0, total_entities, batch_size), desc="æ’å…¥æ•°æ®"):
            end_idx = min(i + batch_size, total_entities)
            
            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            batch_entities = [entity[i:end_idx] for entity in entities]
            
            # æ’å…¥æ•°æ®
            result = self.vector_store.insert_vectors(self.collection_name, batch_entities)
            if not result:
                logger.error(f"æ‰¹æ¬¡ {i//batch_size + 1} æ’å…¥å¤±è´¥")
            
        logger.info("æ•°æ®æ’å…¥å®Œæˆ")
        
        # è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯
        stats = self.vector_store.get_collection_stats(self.collection_name)
        logger.info(f"é›†åˆç»Ÿè®¡ä¿¡æ¯: {stats}")
    
    def run(self):
        """è¿è¡Œé‡æ–°ç´¢å¼•æµç¨‹"""
        try:
            logger.info("å¼€å§‹é‡æ–°ç´¢å¼•æµç¨‹...")
            
            # åŠ è½½æ‰€æœ‰chunks
            chunks = self.load_all_chunks()
            
            if not chunks:
                logger.error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
                return
            
            # åˆ›å»ºé›†åˆå¹¶å¡«å……æ•°æ®
            self.create_and_populate_collection(chunks)
            
            logger.info("é‡æ–°ç´¢å¼•å®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"é‡æ–°ç´¢å¼•å¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    try:
        indexer = VectorIndexer()
        indexer.run()
    except Exception as e:
        logger.error(f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    try:
        indexer = VectorIndexer()
        chunks = indexer.load_all_chunks()
        
        # ç›´æ¥ä»ç¼“å­˜åˆ›å»ºå®ä½“å¹¶æ’å…¥Milvus
        logger.info("å¼€å§‹ä»ç¼“å­˜åˆ›å»ºå®ä½“å¹¶æ’å…¥Milvus...")
        
        # åˆ›å»ºé›†åˆå¹¶å¡«å……æ•°æ®
        indexer.create_and_populate_collection(chunks)
        
        logger.info("å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)